{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k0qGq0RrcOai"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive/\", force_remount=True) \n",
        "colab_path = (\"/content/drive/My Drive/colab/final_project/\")\n",
        "sys.path.append(colab_path)\n",
        "\n",
        "IMG_PATH = \"/content/drive/My Drive/colab/final_project/data/images\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAg1kfoP7tlZ",
        "outputId": "7f010bca-e310-4a1d-a38a-19a55d548934"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code and Concept Mostly based on: https://keras.io/examples/vision/mlp_image_classification/#build-a-classification-model"
      ],
      "metadata": {
        "id": "ocBmU9qTi-_e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8lPoVMKTcOau"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class Patches(layers.Layer):\n",
        "    \"\"\"\n",
        "    https://www.tensorflow.org/api_docs/python/tf/image/extract_patches\n",
        "\n",
        "    For an image, extract square 'patches' of pixels in regular, deterministic\n",
        "    pattern. \n",
        "\n",
        "    Note that patch extraction has no learnable parameters, so it is not a\n",
        "    dynamic part of the network. \n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size, num_patches):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        \n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FNetLayer(layers.Layer):\n",
        "    \"\"\"\n",
        "    https://arxiv.org/abs/2105.03824\n",
        "\n",
        "    FNet: Mixing Tokens with Fourier Transforms\n",
        "\n",
        "    We show that Transformer encoder architectures can be sped up, with \n",
        "    limited accuracy costs, by replacing the self-attention sublayers \n",
        "    with simple linear transformations that \"mix\" input tokens.\n",
        "\n",
        "    ...\n",
        "\n",
        "    FNet has a light memory footprint and is particularly efficient at \n",
        "    smaller model sizes; for a fixed speed and accuracy budget, \n",
        "    small FNet models outperform Transformer counterparts.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
        "        super(FNetLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embedding_dim, activation='relu'),\n",
        "                layers.Dropout(rate=dropout_rate),\n",
        "                layers.Dense(units=embedding_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # extract features using convolution\n",
        "        # Apply fourier transformations.\n",
        "        x = tf.cast(\n",
        "            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
        "            dtype=tf.dtypes.float32,\n",
        "        )\n",
        "\n",
        "        # Add skip connection.\n",
        "        x = x + inputs\n",
        "        # Apply layer normalization.\n",
        "        x = self.normalize1(x)\n",
        "        # Apply Feedfowrad network.\n",
        "        x_ffn = self.ffn(x)\n",
        "        # Add skip connection.\n",
        "        x = x + x_ffn\n",
        "        # Apply layer normalization.\n",
        "        return self.normalize2(x)"
      ],
      "metadata": {
        "id": "Q9cbtsgShmlP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0v_D9tYHcOav"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, num_classes, patch_size=8, num_blocks=4, dropout_rate=0.2, embedding_dim=256):\n",
        "    # single image dimensions\n",
        "    width, height, channels = input_shape\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    num_strides = 2\n",
        "\n",
        "    # Convolution layers\n",
        "    x = tf.keras.layers.Conv2D(filters=3, kernel_size=3, activation='relu', padding='same')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.3)(x)\n",
        "\n",
        "    # --- stride layers ---\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.1)(x)\n",
        "    # -------\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.1)(x)\n",
        "    \n",
        "\n",
        "    # larger convolutions once image is smaller\n",
        "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(.1)(x)\n",
        "\n",
        "    # number of patches depends on desired size of path relative to\n",
        "    # image AFTER convolution is applied\n",
        "    num_patches = ((width//2**num_strides) // patch_size) ** 2  \n",
        "\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size, num_patches)(x)\n",
        "\n",
        "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
        "    x = layers.Dense(units=embedding_dim)(patches)\n",
        "\n",
        "    # use positional encoding for FNet\n",
        "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "    position_embedding = layers.Embedding(\n",
        "        input_dim=num_patches, output_dim=embedding_dim\n",
        "    )(positions)\n",
        "    x = x + position_embedding\n",
        "    \n",
        "    # Process patches using n FNets\n",
        "    fnet_blocks = keras.Sequential(\n",
        "        [\n",
        "            FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)\n",
        "        ]\n",
        "    )\n",
        "    x = fnet_blocks(x)\n",
        "\n",
        "    # Apply global average pooling to generate a [batch_size, embedding_dim] \n",
        "    # representation tensor.\n",
        "    representation = layers.GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    # Apply dropout.\n",
        "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
        "    \n",
        "    # Compute logits outputs.\n",
        "    logits = layers.Dense(num_classes, activation='softmax')(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    return keras.Model(inputs=inputs, outputs=logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">To avoid biasing the annotation for easily classifiable cell images, separate classes were included for artefacts, cells that could not be identified, and other cells belonging to morphological classes not represented in the scheme. From the annotated regions, 250 x 250-pixel images were extracted containing the respective annotated cell as a main content in the patch center (Figure 1A). No further cropping, filtering, or segmentation between foreground and background took place, leaving the algorithm with the task of identifying the main image content relevant for the respective annotation.\n",
        "\n",
        "- Matek, Krappe, et. al pp. 1918, \"Highly accurate differentiation of bone marrow cell\n",
        "morphologies using deep neural networks on a large image\n",
        "data set\""
      ],
      "metadata": {
        "id": "6_9xPiqHlCdO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5taEljCccOax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a2aebf-c29d-465a-cdff-a3de712afc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 66438 files belonging to 21 classes.\n",
            "Using 53151 files for training.\n",
            "Found 66438 files belonging to 21 classes.\n",
            "Using 13287 files for validation.\n"
          ]
        }
      ],
      "source": [
        "# Properties of our dataset\n",
        "IMG_DIM = 128\n",
        "BATCH_SIZE = 200\n",
        "IMAGE_SHAPE = (IMG_DIM, IMG_DIM, 3)\n",
        "\n",
        "# get all the data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=IMG_PATH,\n",
        "    label_mode='categorical',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=(IMG_DIM, IMG_DIM),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=IMG_PATH,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    label_mode='categorical',\n",
        "    seed=1337,\n",
        "    image_size=(IMG_DIM, IMG_DIM),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSS9WAcxcOax",
        "outputId": "263dde41-1406-4fb3-fd3a-7b0f638e2a69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 128, 128, 3)       84        \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 128, 128, 3)      12        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 128, 128, 3)       0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 64, 64, 64)        1792      \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64, 64, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_24 (Dropout)        (None, 64, 64, 64)        0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_25 (Dropout)        (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 32, 32, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 32, 32, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_26 (Dropout)        (None, 32, 32, 64)        0         \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 32, 32, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 32, 32, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_27 (Dropout)        (None, 32, 32, 128)       0         \n",
            "                                                                 \n",
            " conv2d_18 (Conv2D)          (None, 32, 32, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 32, 32, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " patches_2 (Patches)         (None, 16, 16384)         0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 16, 256)           4194560   \n",
            "                                                                 \n",
            " tf.__operators__.add_2 (TFO  (None, 16, 256)          0         \n",
            " pLambda)                                                        \n",
            "                                                                 \n",
            " sequential_14 (Sequential)  (None, 16, 256)           530432    \n",
            "                                                                 \n",
            " global_average_pooling1d_2   (None, 256)              0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 21)                5397      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,177,461\n",
            "Trainable params: 5,176,303\n",
            "Non-trainable params: 1,158\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Size of the patches in pixels be extracted from convolved features\n",
        "# ~0.25 of side length worked well with 32x32\n",
        "PATCH_SIZE = 8\n",
        "\n",
        "# Number of FNET blocks\n",
        "# More blocks greatly decreases training time\n",
        "NUM_BLOCKS = 4\n",
        "\n",
        "# Number of hidden units in each FNET block\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_CLASSES = 21\n",
        "\n",
        "model = build_model(\n",
        "    input_shape=IMAGE_SHAPE, \n",
        "    num_classes=NUM_CLASSES, \n",
        "    patch_size=PATCH_SIZE, \n",
        "    num_blocks=NUM_BLOCKS, \n",
        "    embedding_dim=HIDDEN_SIZE\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PERFORMANCE_METRICS = [\n",
        "    # make sure your classes are one-hot encoded\n",
        "    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "    tf.keras.metrics.Precision(name='precision'),\n",
        "    tf.keras.metrics.Recall(name='recall'),\n",
        "    tf.keras.metrics.AUC(name='auc'),\n",
        "    # precision recall curve\n",
        "    tf.keras.metrics.AUC(name='prc', curve='PR'), \n",
        "]\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=MODEL_PERFORMANCE_METRICS,\n",
        ")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs = 20,\n",
        "    validation_data=val_ds,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR2aSkPRiw1-",
        "outputId": "6cbc0ef5-47b8-4e92-86ec-54b446ff4e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "266/266 [==============================] - 563s 2s/step - loss: 1.3065 - accuracy: 0.5465 - precision: 0.6969 - recall: 0.3809 - auc: 0.9547 - prc: 0.5931 - val_loss: 2.2289 - val_accuracy: 0.2959 - val_precision: 0.3162 - val_recall: 0.2395 - val_auc: 0.9059 - val_prc: 0.2898\n",
            "Epoch 2/20\n",
            "266/266 [==============================] - 143s 528ms/step - loss: 0.9048 - accuracy: 0.6938 - precision: 0.7860 - recall: 0.6062 - auc: 0.9771 - prc: 0.7713 - val_loss: 1.1483 - val_accuracy: 0.5674 - val_precision: 0.6519 - val_recall: 0.4927 - val_auc: 0.9670 - val_prc: 0.6576\n",
            "Epoch 3/20\n",
            "266/266 [==============================] - 141s 521ms/step - loss: 0.7026 - accuracy: 0.7659 - precision: 0.8326 - recall: 0.7065 - auc: 0.9847 - prc: 0.8482 - val_loss: 1.1659 - val_accuracy: 0.6288 - val_precision: 0.6804 - val_recall: 0.5893 - val_auc: 0.9640 - val_prc: 0.7098\n",
            "Epoch 4/20\n",
            "266/266 [==============================] - 144s 532ms/step - loss: 0.5919 - accuracy: 0.8036 - precision: 0.8550 - recall: 0.7561 - auc: 0.9884 - prc: 0.8846 - val_loss: 1.0136 - val_accuracy: 0.6659 - val_precision: 0.7205 - val_recall: 0.6123 - val_auc: 0.9697 - val_prc: 0.7401\n",
            "Epoch 5/20\n",
            "266/266 [==============================] - 143s 526ms/step - loss: 0.5172 - accuracy: 0.8265 - precision: 0.8690 - recall: 0.7879 - auc: 0.9906 - prc: 0.9072 - val_loss: 0.7769 - val_accuracy: 0.7437 - val_precision: 0.7975 - val_recall: 0.7029 - val_auc: 0.9797 - val_prc: 0.8276\n",
            "Epoch 6/20\n",
            "266/266 [==============================] - 142s 523ms/step - loss: 0.4597 - accuracy: 0.8473 - precision: 0.8813 - recall: 0.8160 - auc: 0.9921 - prc: 0.9228 - val_loss: 0.9815 - val_accuracy: 0.7072 - val_precision: 0.7399 - val_recall: 0.6775 - val_auc: 0.9679 - val_prc: 0.7877\n",
            "Epoch 7/20\n",
            "266/266 [==============================] - 142s 523ms/step - loss: 0.4056 - accuracy: 0.8639 - precision: 0.8924 - recall: 0.8370 - auc: 0.9936 - prc: 0.9371 - val_loss: 0.8731 - val_accuracy: 0.7400 - val_precision: 0.7636 - val_recall: 0.7200 - val_auc: 0.9723 - val_prc: 0.8151\n",
            "Epoch 8/20\n",
            "266/266 [==============================] - 141s 519ms/step - loss: 0.3607 - accuracy: 0.8792 - precision: 0.9029 - recall: 0.8570 - auc: 0.9943 - prc: 0.9478 - val_loss: 0.7005 - val_accuracy: 0.7933 - val_precision: 0.8125 - val_recall: 0.7778 - val_auc: 0.9786 - val_prc: 0.8666\n",
            "Epoch 9/20\n",
            "266/266 [==============================] - 141s 522ms/step - loss: 0.3149 - accuracy: 0.8940 - precision: 0.9137 - recall: 0.8754 - auc: 0.9957 - prc: 0.9586 - val_loss: 0.9214 - val_accuracy: 0.7480 - val_precision: 0.7681 - val_recall: 0.7336 - val_auc: 0.9666 - val_prc: 0.8178\n",
            "Epoch 10/20\n",
            "266/266 [==============================] - 142s 521ms/step - loss: 0.2710 - accuracy: 0.9084 - precision: 0.9239 - recall: 0.8944 - auc: 0.9965 - prc: 0.9677 - val_loss: 0.7483 - val_accuracy: 0.7856 - val_precision: 0.8053 - val_recall: 0.7719 - val_auc: 0.9743 - val_prc: 0.8569\n",
            "Epoch 11/20\n",
            "266/266 [==============================] - 142s 524ms/step - loss: 0.2425 - accuracy: 0.9182 - precision: 0.9309 - recall: 0.9057 - auc: 0.9969 - prc: 0.9728 - val_loss: 0.8707 - val_accuracy: 0.7837 - val_precision: 0.7988 - val_recall: 0.7753 - val_auc: 0.9647 - val_prc: 0.8412\n",
            "Epoch 12/20\n",
            "266/266 [==============================] - 143s 528ms/step - loss: 0.2056 - accuracy: 0.9291 - precision: 0.9394 - recall: 0.9197 - auc: 0.9977 - prc: 0.9792 - val_loss: 0.9042 - val_accuracy: 0.7737 - val_precision: 0.7859 - val_recall: 0.7654 - val_auc: 0.9645 - val_prc: 0.8376\n",
            "Epoch 13/20\n",
            "266/266 [==============================] - 144s 530ms/step - loss: 0.1783 - accuracy: 0.9393 - precision: 0.9471 - recall: 0.9318 - auc: 0.9981 - prc: 0.9838 - val_loss: 0.9515 - val_accuracy: 0.7867 - val_precision: 0.7978 - val_recall: 0.7792 - val_auc: 0.9593 - val_prc: 0.8331\n",
            "Epoch 14/20\n",
            "266/266 [==============================] - 142s 524ms/step - loss: 0.1586 - accuracy: 0.9446 - precision: 0.9509 - recall: 0.9391 - auc: 0.9984 - prc: 0.9864 - val_loss: 1.1147 - val_accuracy: 0.7601 - val_precision: 0.7693 - val_recall: 0.7532 - val_auc: 0.9499 - val_prc: 0.7927\n",
            "Epoch 15/20\n",
            "266/266 [==============================] - 142s 524ms/step - loss: 0.1398 - accuracy: 0.9516 - precision: 0.9570 - recall: 0.9477 - auc: 0.9987 - prc: 0.9891 - val_loss: 1.1524 - val_accuracy: 0.7558 - val_precision: 0.7658 - val_recall: 0.7479 - val_auc: 0.9488 - val_prc: 0.7920\n",
            "Epoch 16/20\n",
            "266/266 [==============================] - 141s 521ms/step - loss: 0.1181 - accuracy: 0.9585 - precision: 0.9627 - recall: 0.9552 - auc: 0.9989 - prc: 0.9915 - val_loss: 1.4230 - val_accuracy: 0.7238 - val_precision: 0.7315 - val_recall: 0.7178 - val_auc: 0.9343 - val_prc: 0.7370\n",
            "Epoch 17/20\n",
            "266/266 [==============================] - 139s 513ms/step - loss: 0.1088 - accuracy: 0.9614 - precision: 0.9653 - recall: 0.9587 - auc: 0.9991 - prc: 0.9929 - val_loss: 1.2885 - val_accuracy: 0.7519 - val_precision: 0.7595 - val_recall: 0.7469 - val_auc: 0.9422 - val_prc: 0.7737\n",
            "Epoch 18/20\n",
            "266/266 [==============================] - 141s 520ms/step - loss: 0.1028 - accuracy: 0.9644 - precision: 0.9680 - recall: 0.9619 - auc: 0.9991 - prc: 0.9932 - val_loss: 1.1514 - val_accuracy: 0.7718 - val_precision: 0.7799 - val_recall: 0.7666 - val_auc: 0.9488 - val_prc: 0.8007\n",
            "Epoch 19/20\n",
            "266/266 [==============================] - 141s 519ms/step - loss: 0.0865 - accuracy: 0.9691 - precision: 0.9715 - recall: 0.9670 - auc: 0.9993 - prc: 0.9951 - val_loss: 1.0284 - val_accuracy: 0.7923 - val_precision: 0.8009 - val_recall: 0.7869 - val_auc: 0.9549 - val_prc: 0.8278\n",
            "Epoch 20/20\n",
            "174/266 [==================>...........] - ETA: 39s - loss: 0.0815 - accuracy: 0.9729 - precision: 0.9748 - recall: 0.9710 - auc: 0.9992 - prc: 0.9951"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ae4jJaTBZ3cd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}