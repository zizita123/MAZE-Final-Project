{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "k0qGq0RrcOai"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P04ng0AaiP9u"
   },
   "source": [
    "Install Tensorflow Addons: [LINK](https://colab.research.google.com/github/tensorflow/addons/blob/master/docs/tutorials/image_ops.ipynb#scrollTo=o_QTX_vHGbj7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrKeIqx7iNFv",
    "outputId": "1001f8bc-b8a4-40cb-d945-d6481d33ac63"
   },
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAg1kfoP7tlZ",
    "outputId": "8251996a-6769-40b9-9dbe-0e1f1bb83f5a"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# from google.colab import drive \n",
    "# drive.mount(\"/content/drive/\", force_remount=True) \n",
    "# colab_path = (\"/content/drive/My Drive/colab/final_project/\")\n",
    "# sys.path.append(colab_path)\n",
    "\n",
    "IMG_PATH = \"./data/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFuO859TghA4",
    "outputId": "7c561918-82ca-448d-96af-40479ed1f163"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# this doesn't work if the images are shared with you but keras can still \n",
    "# load them\n",
    "glob.glob(IMG_PATH + \"*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocBmU9qTi-_e"
   },
   "source": [
    "Code and Concept Mostly based on: https://keras.io/examples/vision/mlp_image_classification/#build-a-classification-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "8lPoVMKTcOau"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x7cLyAuFeVJK"
   },
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    \"\"\"\n",
    "    https://www.tensorflow.org/api_docs/python/tf/image/extract_patches\n",
    "\n",
    "    For an image, extract square 'patches' of pixels in regular, deterministic\n",
    "    pattern. \n",
    "\n",
    "    Note that patch extraction has no learnable parameters, so it is not a\n",
    "    dynamic part of the network. \n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, num_patches):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        \n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q9cbtsgShmlP"
   },
   "outputs": [],
   "source": [
    "class FNetLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    https://arxiv.org/abs/2105.03824\n",
    "\n",
    "    FNet: Mixing Tokens with Fourier Transforms\n",
    "\n",
    "    We show that Transformer encoder architectures can be sped up, with \n",
    "    limited accuracy costs, by replacing the self-attention sublayers \n",
    "    with simple linear transformations that \"mix\" input tokens.\n",
    "\n",
    "    ...\n",
    "\n",
    "    FNet has a light memory footprint and is particularly efficient at \n",
    "    smaller model sizes; for a fixed speed and accuracy budget, \n",
    "    small FNet models outperform Transformer counterparts.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n",
    "        super(FNetLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.ffn = keras.Sequential(\n",
    "            [ \n",
    "                # RELU works better than GELU\n",
    "                layers.Dense(units=embedding_dim, activation='relu'),\n",
    "                # tfa.layers.GELU(),\n",
    "                layers.Dropout(rate=dropout_rate),\n",
    "                layers.Dense(units=embedding_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # extract features using convolution\n",
    "        # Apply fourier transformations.\n",
    "        x = tf.cast(\n",
    "            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n",
    "            dtype=tf.dtypes.float32,\n",
    "        )\n",
    "\n",
    "        # Add skip connection.\n",
    "        x = x + inputs\n",
    "        # Apply layer normalization.\n",
    "        x = self.normalize1(x)\n",
    "        # Apply Feedfowrad network.\n",
    "        x_ffn = self.ffn(x)\n",
    "        # Add skip connection.\n",
    "        x = x + x_ffn\n",
    "        # Apply layer normalization.\n",
    "        return self.normalize2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_9xPiqHlCdO"
   },
   "source": [
    ">To avoid biasing the annotation for easily classifiable cell images, separate classes were included for artefacts, cells that could not be identified, and other cells belonging to morphological classes not represented in the scheme. From the annotated regions, 250 x 250-pixel images were extracted containing the respective annotated cell as a main content in the patch center (Figure 1A). No further cropping, filtering, or segmentation between foreground and background took place, leaving the algorithm with the task of identifying the main image content relevant for the respective annotation.\n",
    "\n",
    "- Matek, Krappe, et. al pp. 1918, \"Highly accurate differentiation of bone marrow cell\n",
    "morphologies using deep neural networks on a large image\n",
    "data set\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5taEljCccOax"
   },
   "outputs": [],
   "source": [
    "# # Properties of our dataset\n",
    "# IMG_DIM = 128\n",
    "# BATCH_SIZE = 200\n",
    "# IMAGE_SHAPE = (IMG_DIM, IMG_DIM, 3)\n",
    "\n",
    "# # get all the data\n",
    "# train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=IMG_PATH,\n",
    "#     label_mode='categorical',\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"training\",\n",
    "#     seed=1337,\n",
    "#     image_size=(IMG_DIM, IMG_DIM),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "# )\n",
    "# val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#     directory=IMG_PATH,\n",
    "#     validation_split=0.2,\n",
    "#     subset=\"validation\",\n",
    "#     label_mode='categorical',\n",
    "#     seed=1337,\n",
    "#     image_size=(IMG_DIM, IMG_DIM),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "# )\n",
    "\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = \"/data/images/\"\n",
    "IMAGE_PATH = os.getcwd() + DATA_DIR\n",
    "\n",
    "T_TEST_TRAIN = Tuple[np.array, np.array, np.array, np.array]\n",
    "\n",
    "class ImagePot:\n",
    "    # parsed from abbreviations.csv\n",
    "    CLASSNAME_LOOKUP = {\n",
    "        'ABE': 'Abnormal eosinophil',\n",
    "        'ART': 'Artefact',\n",
    "        'BAS': 'Basophil',\n",
    "        'BLA': 'Blast',\n",
    "        'EBO': 'Erythroblast',\n",
    "        'EOS': 'Eosinophil',\n",
    "        'FGC': 'Faggott cell',\n",
    "        'HAC': 'Hairy cell',\n",
    "        'KSC': 'Smudge cell',\n",
    "        'LYI': 'Immature lymphocyte',\n",
    "        'LYT': 'Lymphocyte',\n",
    "        'MMZ': 'Metamyelocyte',\n",
    "        'MON': 'Monocyte',\n",
    "        'MYB': 'Myelocyte',\n",
    "        'NGB': 'Band neutrophil',\n",
    "        'NGS': 'Segmented neutrophil',\n",
    "        'NIF': 'Not identifiable',\n",
    "        'OTH': 'Other cell',\n",
    "        'PEB': 'Proerythroblast',\n",
    "        'PLM': 'Plasma cell',\n",
    "        'PMO': 'Promyelocyte',\n",
    "    }\n",
    "\n",
    "    def __init__(self, image_paths: List[str], classname: str, encoding: Optional[List[int]] = None) -> ImagePot:\n",
    "        self.image_paths = image_paths\n",
    "        self.classname = classname\n",
    "        self.encoding = encoding\n",
    "        self.full_name = self.CLASSNAME_LOOKUP.get(classname, \"?\")\n",
    "\n",
    "    def set_encoding(self, encoding) -> None:\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def get_test_train_split(self, split: float = 0.8) -> T_TEST_TRAIN:\n",
    "        n = len(self.image_paths)\n",
    "        div = int(n * split)\n",
    "\n",
    "        labels = [self.encoding] * n\n",
    "\n",
    "        # split them up\n",
    "        train_labels, test_labels = np.array(labels[:div]), np.array(labels[div:])\n",
    "        train, test = self.image_paths[:div], self.image_paths[div:]\n",
    "        \n",
    "        # load the images\n",
    "        train = np.array([cv2.imread(x).astype(np.float32) for x in train])\n",
    "        test = np.array([cv2.imread(x).astype(np.float32) for x in test])\n",
    "\n",
    "        # convention in our assignments\n",
    "        X0, X1 = train, test\n",
    "        Y0, Y1 = train_labels, test_labels\n",
    "        return X0, Y0, X1, Y1\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return \"Image Pot containing Class {0} ({1}), {2} file(s)\".format(\n",
    "            self.classname, \n",
    "            self.full_name,\n",
    "            len(self.image_paths),\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_paths)\n",
    "\n",
    "def load_images(\n",
    "        num_image_minimum: int = 100, \n",
    "        num_image_limit: Optional[int] = 200,\n",
    "        print_files_on_disk: bool = False,\n",
    "        load_classes: List[str] = None,\n",
    "    ) -> Dict[str, Dict[str, object]]: \n",
    "\n",
    "    # these are the labels enclosing each folder of images\n",
    "    sub_dirs = sorted([y for x in os.walk(IMAGE_PATH) if (y := (x[0].split(IMAGE_PATH)[-1]))])\n",
    "\n",
    "    load_classes = load_classes or sub_dirs\n",
    "\n",
    "    # ....\n",
    "    sub_dirs = sorted(load_classes)\n",
    "\n",
    "    # assume that data processing script has already un-nested image contents\n",
    "    good_folders = {}\n",
    "\n",
    "    for folder in sub_dirs:\n",
    "        file_names = os.listdir(\"/\".join([IMAGE_PATH, folder]))\n",
    "        num_images = len(file_names)\n",
    "        if print_files_on_disk:\n",
    "            print(f\"Classname: {folder} has {num_images} images.\")\n",
    "\n",
    "        if num_images >= num_image_minimum:\n",
    "            good_folders[folder] = {\n",
    "                'pot': ImagePot([IMAGE_PATH + folder + \"/\" + x for x in file_names[:num_image_limit]], folder), \n",
    "                'classname': folder,\n",
    "                'ohe': None,\n",
    "            }\n",
    "        else:\n",
    "            print(f\"WARNING: Classname: {folder} had fewer than {num_image_minimum} images. Had {num_images}\")\n",
    "\n",
    "    # directory of valid images\n",
    "    ohe_classes = [([0] * len(good_folders)) for _ in range(len(good_folders))]\n",
    "\n",
    "    for i, k in enumerate(good_folders):\n",
    "        encoding = ohe_classes[i]\n",
    "        # set equal to available classes\n",
    "        encoding[i] = 1\n",
    "        good_folders[k]['pot'].set_encoding(encoding)\n",
    "\n",
    "        # simplify repo structure\n",
    "        good_folders[k] = good_folders[k]['pot']\n",
    "\n",
    "    return good_folders\n",
    "\n",
    "def get_nn_data(\n",
    "        data_repo: Dict, \n",
    "        load_classes: List[str] = None,\n",
    "        test_train_split: float = 0.8, \n",
    "        shuffle: bool = False\n",
    ") -> T_TEST_TRAIN:\n",
    "    # default to all classes\n",
    "    load_classes = load_classes or list(data_repo.keys())\n",
    "\n",
    "    # train\n",
    "    X0 = np.array([])\n",
    "    Y0 = np.array([])\n",
    "\n",
    "    # test\n",
    "    X1 = np.array([])\n",
    "    Y1 = np.array([])\n",
    "    \n",
    "    total_classes = len(load_classes)\n",
    "    for i, x in enumerate(data_repo.items()):\n",
    "        class_name, pot = x\n",
    "        \n",
    "        if class_name not in load_classes:\n",
    "            # skip it\n",
    "            continue\n",
    "            \n",
    "        total_images = len(pot)\n",
    "        print(\"Loading {} images in {} ({}/{})\".format(\n",
    "            total_images, \n",
    "            class_name, \n",
    "            i + 1, \n",
    "            total_classes\n",
    "        ))\n",
    "\n",
    "        x0, y0, x1, y1 = pot.get_test_train_split(test_train_split)\n",
    "        # TRAIN\n",
    "        X0 = x0 if X0.shape[0] == 0 else np.append(X0, x0, axis=0)\n",
    "        Y0 = y0 if Y0.shape[0] == 0 else np.append(Y0, y0, axis=0)\n",
    "\n",
    "        # TEST\n",
    "        X1 = x1 if X1.shape[0] == 0 else np.append(X1, x1, axis=0)\n",
    "        Y1 = y1 if Y1.shape[0] == 0 else np.append(Y1, y1, axis=0)\n",
    "\n",
    "    print(\"loaded the following: X0: {0}, Y0: {1}, X1: {2}, Y1: {3}\". format(\n",
    "        X0.shape,\n",
    "        Y0.shape,\n",
    "        X1.shape,\n",
    "        Y1.shape,\n",
    "    ))\n",
    "\n",
    "    X0, Y0 = shuffle_data_and_labels(X0, Y0)\n",
    "    X1, Y1 = shuffle_data_and_labels(X1, Y1)\n",
    "\n",
    "    return X0, Y0, X1, Y1, load_classes\n",
    "\n",
    "def shuffle_data_and_labels(data: np.array, labels: np.array) -> Tuple[np.array, np.array]:\n",
    "    x = data.shape[0]\n",
    "    y = labels.shape[0]\n",
    "\n",
    "    if x != y:\n",
    "        raise ValueError(\"Mismatch between number of data and number of labels\")\n",
    "\n",
    "    idx = tf.random.shuffle(tf.range(start=0, limit=x, dtype=tf.int32))\n",
    "    s_d = tf.gather(data, idx)\n",
    "    s_l = tf.gather(labels, idx)\n",
    "    return s_d, s_l\n",
    "\n",
    "def load_data_for_training(min_images: int, max_images: int, load_classes: List[str]) -> T_TEST_TRAIN:\n",
    "    images = load_images(\n",
    "        num_image_minimum=min_images,\n",
    "        num_image_limit=max_images, \n",
    "        load_classes=load_classes,\n",
    "    )\n",
    "    return get_nn_data(images, load_classes, 0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Classname: ABE had fewer than 100 images. Had 8\n",
      "WARNING: Classname: FGC had fewer than 100 images. Had 47\n",
      "WARNING: Classname: KSC had fewer than 100 images. Had 42\n",
      "WARNING: Classname: LYI had fewer than 100 images. Had 65\n",
      "Loading 100 images in ART (1/17)\n",
      "Loading 100 images in BAS (2/17)\n",
      "Loading 100 images in BLA (3/17)\n",
      "Loading 100 images in EBO (4/17)\n",
      "Loading 100 images in EOS (5/17)\n",
      "Loading 100 images in HAC (6/17)\n",
      "Loading 100 images in LYT (7/17)\n",
      "Loading 100 images in MMZ (8/17)\n",
      "Loading 100 images in MON (9/17)\n",
      "Loading 100 images in MYB (10/17)\n",
      "Loading 100 images in NGB (11/17)\n",
      "Loading 100 images in NGS (12/17)\n",
      "Loading 100 images in NIF (13/17)\n",
      "Loading 100 images in OTH (14/17)\n",
      "Loading 100 images in PEB (15/17)\n",
      "Loading 100 images in PLM (16/17)\n",
      "Loading 100 images in PMO (17/17)\n",
      "loaded the following: X0: (1360, 250, 250, 3), Y0: (1360, 17), X1: (340, 250, 250, 3), Y1: (340, 17)\n"
     ]
    }
   ],
   "source": [
    "IMG_DIM = 128\n",
    "\n",
    "X0, Y0, X1, Y1, loaded_classes = load_data_for_training(\n",
    "    min_images=100,\n",
    "    max_images=100,\n",
    "    load_classes=None,\n",
    ")\n",
    "\n",
    "image_preprocess = tf.keras.Sequential(\n",
    "  [\n",
    "      tf.keras.layers.Resizing(IMG_DIM, IMG_DIM),\n",
    "      tf.keras.layers.Rescaling(1./255),\n",
    "  ],\n",
    ")\n",
    "\n",
    "X0 = image_preprocess(X0)\n",
    "X1 = image_preprocess(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 171364 files belonging to 21 classes.\n",
      "Using 137092 files for training.\n",
      "Found 171364 files belonging to 21 classes.\n",
      "Using 34272 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=IMG_PATH,\n",
    "    label_mode='categorical',\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=1337,\n",
    "    image_size=(IMG_DIM, IMG_DIM),\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=IMG_PATH,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    label_mode='categorical',\n",
    "    seed=1337,\n",
    "    image_size=(IMG_DIM, IMG_DIM),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18\n",
    "\"\"\"\n",
    "\n",
    "def func(i):\n",
    "    i = i.numpy() # Decoding from the EagerTensor object\n",
    "    x, y = train_ds[i]\n",
    "    return x, y\n",
    "\n",
    "def _fixup_shape(x, y):\n",
    "    nb_channels = 3\n",
    "    nb_classes = 21\n",
    "    x.set_shape([None, None, None, nb_channels]) # n, h, w, c\n",
    "    y.set_shape([None, nb_classes]) # n, nb_classes\n",
    "    return x, y\n",
    "\n",
    "z = list(range(len(train_ds))) # The index generator\n",
    "dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8)\n",
    "   \n",
    "dataset = dataset.shuffle(buffer_size=len(z), seed=0,  \n",
    "                          reshuffle_each_iteration=True)\n",
    "dataset = dataset.map(lambda i: tf.py_function(func=func, \n",
    "                                               inp=[i], \n",
    "                                               Tout=[tf.uint8,\n",
    "                                                     tf.float32]\n",
    "                                               ), \n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(8).map(_fixup_shape)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSS9WAcxcOax",
    "outputId": "320ed16a-9cbf-4818-b739-0fd10466ccb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_40 (InputLayer)       [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_98 (Conv2D)          (None, 128, 128, 3)       84        \n",
      "                                                                 \n",
      " batch_normalization_109 (Ba  (None, 128, 128, 3)      12        \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_204 (Dropout)       (None, 128, 128, 3)       0         \n",
      "                                                                 \n",
      " conv2d_99 (Conv2D)          (None, 64, 64, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_110 (Ba  (None, 64, 64, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_205 (Dropout)       (None, 64, 64, 64)        0         \n",
      "                                                                 \n",
      " conv2d_100 (Conv2D)         (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_111 (Ba  (None, 32, 32, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_206 (Dropout)       (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_101 (Conv2D)         (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_112 (Ba  (None, 32, 32, 64)       256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_207 (Dropout)       (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_102 (Conv2D)         (None, 32, 32, 256)       147712    \n",
      "                                                                 \n",
      " batch_normalization_113 (Ba  (None, 32, 32, 256)      1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_208 (Dropout)       (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_103 (Conv2D)         (None, 32, 32, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_114 (Ba  (None, 32, 32, 128)      512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_209 (Dropout)       (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " patches_19 (Patches)        (None, 16, 8192)          0         \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 16, 32)            786464    \n",
      "                                                                 \n",
      " batch_normalization_115 (Ba  (None, 16, 32)           128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_210 (Dropout)       (None, 16, 32)            0         \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 16, 32)            3104      \n",
      "                                                                 \n",
      " dense_190 (Dense)           (None, 16, 256)           8448      \n",
      "                                                                 \n",
      " tf.__operators__.add_19 (TF  (None, 16, 256)          0         \n",
      " OpLambda)                                                       \n",
      "                                                                 \n",
      " sequential_122 (Sequential)  (None, 16, 256)          530432    \n",
      "                                                                 \n",
      " global_average_pooling1d_19  (None, 256)              0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_215 (Dropout)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_199 (Dense)           (None, 21)                5397      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,854,773\n",
      "Trainable params: 1,853,551\n",
      "Non-trainable params: 1,222\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Size of the patches in pixels be extracted from convolved features\n",
    "# ~0.25 of side length worked well with 32x32\n",
    "PATCH_SIZE = 8\n",
    "BATCH_SIZE = 200\n",
    "IMAGE_SHAPE = (IMG_DIM, IMG_DIM, 3)\n",
    "\n",
    "# Number of FNET blocks\n",
    "# More blocks greatly decreases training time\n",
    "NUM_BLOCKS = 4\n",
    "\n",
    "# Number of hidden units in each FNET block\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_CLASSES = 21\n",
    "\n",
    "def build_model(input_shape, num_classes, patch_size=8, num_blocks=4, dropout_rate=0.2, embedding_dim=256):\n",
    "    # single image dimensions\n",
    "    width, height, channels = input_shape\n",
    "    \n",
    "    data_augmentation = tf.keras.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.Rescaling(1./255),\n",
    "          tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "          tf.keras.layers.RandomZoom(\n",
    "              height_factor=0.2, width_factor=0.2\n",
    "          ),\n",
    "          # we expect these to rotate a lot, how do you know\n",
    "          # which is the 'bottom' of a cell?\n",
    "          tf.keras.layers.RandomRotation(\n",
    "              factor=(-0.7, 0.6)\n",
    "          ),\n",
    "      ],\n",
    "    )\n",
    "    \n",
    "    # original image size on disk\n",
    "    inputs = layers.Input(shape=IMAGE_SHAPE)\n",
    "    num_strides = 2\n",
    "\n",
    "    # add data augmentation\n",
    "    inputs = data_augmentation(inputs)\n",
    "\n",
    "    # Convolution layers\n",
    "    x = tf.keras.layers.Conv2D(filters=3, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.3)(x)\n",
    "\n",
    "    # --- stride layers ---\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.1)(x)\n",
    "\n",
    "    # x = tf.keras.layers.MaxPooling2D(pool_size = (2,2))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=(2, 2), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.1)(x)\n",
    "    # -------\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.1)(x)\n",
    "    \n",
    "    # larger convolutions once image is smaller\n",
    "    x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.1)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(.1)(x)\n",
    "\n",
    "    # number of patches depends on desired size of path relative to\n",
    "    # image AFTER convolution is applied\n",
    "    # num_strides = 0\n",
    "    num_patches = ((width//2**num_strides) // patch_size) ** 2  \n",
    "\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size, num_patches)(x)\n",
    "\n",
    "    # Convolve the patches a few times\n",
    "    patches = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(patches)\n",
    "    patches = tf.keras.layers.BatchNormalization()(patches)\n",
    "    patches = tf.keras.layers.Dropout(.1)(patches)\n",
    "    patches = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(patches)\n",
    "\n",
    "    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n",
    "    x = layers.Dense(units=embedding_dim)(patches)\n",
    "\n",
    "    # use positional encoding for FNet\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    position_embedding = layers.Embedding(\n",
    "        input_dim=num_patches, output_dim=embedding_dim\n",
    "    )(positions)\n",
    "    x = x + position_embedding\n",
    "    \n",
    "    # Process patches using n FNets\n",
    "    fnet_blocks = keras.Sequential(\n",
    "        [\n",
    "            FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)\n",
    "        ]\n",
    "    )\n",
    "    x = fnet_blocks(x)\n",
    "\n",
    "    # Apply global average pooling to generate a [batch_size, embedding_dim] \n",
    "    # representation tensor.\n",
    "    representation = layers.GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Apply dropout.\n",
    "    representation = layers.Dropout(rate=dropout_rate)(representation)\n",
    "    \n",
    "    # Compute logits outputs.\n",
    "    logits = layers.Dense(num_classes, activation='softmax')(representation)\n",
    "\n",
    "    # Create the Keras model.\n",
    "    return keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "model = build_model(\n",
    "    input_shape=IMAGE_SHAPE, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    patch_size=PATCH_SIZE, \n",
    "    num_blocks=NUM_BLOCKS, \n",
    "    embedding_dim=HIDDEN_SIZE\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "MODEL_PERFORMANCE_METRICS = [\n",
    "    # make sure your classes are one-hot encoded\n",
    "    tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    # precision recall curve\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR'), \n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    # default configuration adam works better than weighted decay adam\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=MODEL_PERFORMANCE_METRICS,\n",
    ")\n",
    "\n",
    "# image transformations\n",
    "    \n",
    "\n",
    "# history = model.fit(\n",
    "#     X0, \n",
    "#     Y0,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs = 10,\n",
    "#     validation_data=(X1, Y1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 123s 2s/step - loss: 2.4642 - accuracy: 0.1912 - precision: 1.0000 - recall: 6.2500e-04 - auc: 0.8080 - prc: 0.1525 - val_loss: 2.4697 - val_accuracy: 0.1294 - val_precision: 0.6752 - val_recall: 0.0116 - val_auc: 0.8190 - val_prc: 0.1719\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    epochs = 1,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 2.2411 - accuracy: 0.2719 - precision: 0.4706 - recall: 0.0200 - auc: 0.8484 - prc: 0.2274"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    epochs = 1,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    epochs = 20,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "DL3 (3.9)",
   "language": "python",
   "name": "dl3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
